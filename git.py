import os
import json
from pathlib import Path
from collections import defaultdict

class ChromaDBAnalyzer:
    def __init__(self, chroma_path="./chroma_db"):
        self.chroma_path = Path(chroma_path)
        self.file_stats = defaultdict(list)
        self.large_files = []
        self.total_size = 0
        
    def analyze_directory(self):
        """Analisa todos os arquivos na pasta chroma_db"""
        print(f"🔍 Analisando pasta: {self.chroma_path}")
        print("=" * 60)
        
        if not self.chroma_path.exists():
            print(f"❌ Pasta {self.chroma_path} não encontrada!")
            return False
            
        for root, dirs, files in os.walk(self.chroma_path):
            for file in files:
                file_path = Path(root) / file
                try:
                    size = file_path.stat().st_size
                    size_mb = size / (1024 * 1024)
                    
                    # Coleta estatísticas
                    extension = file_path.suffix.lower()
                    relative_path = file_path.relative_to(Path("."))
                    
                    file_info = {
                        'name': file,
                        'path': str(relative_path),
                        'size_bytes': size,
                        'size_mb': size_mb,
                        'extension': extension
                    }
                    
                    self.file_stats[extension].append(file_info)
                    self.total_size += size
                    
                    # Arquivos que excedem limite do GitHub (100MB)
                    if size_mb > 100:
                        self.large_files.append(file_info)
                        
                except Exception as e:
                    print(f"⚠️ Erro ao analisar {file_path}: {e}")
        
        return True
    
    def print_analysis(self):
        """Mostra análise detalhada"""
        print("\n📊 ANÁLISE DETALHADA")
        print("=" * 60)
        
        print(f"📁 Pasta analisada: {self.chroma_path}")
        print(f"📏 Tamanho total: {self.total_size / (1024*1024):.2f} MB")
        print(f"🚨 Arquivos > 100MB: {len(self.large_files)}")
        
        print(f"\n📋 ARQUIVOS POR EXTENSÃO:")
        print("-" * 40)
        
        for ext, files in sorted(self.file_stats.items()):
            total_size = sum(f['size_bytes'] for f in files)
            avg_size = total_size / len(files) / (1024*1024)
            total_mb = total_size / (1024*1024)
            
            print(f"{ext or '(sem extensão)':15} | "
                  f"{len(files):3} arquivos | "
                  f"{total_mb:8.2f} MB | "
                  f"Média: {avg_size:.2f} MB")
        
        if self.large_files:
            print(f"\n🚨 ARQUIVOS GRANDES (>100MB):")
            print("-" * 50)
            for file in self.large_files:
                print(f"  📄 {file['name']}: {file['size_mb']:.2f} MB")
                print(f"     Caminho: {file['path']}")
    
    def generate_gitattributes(self):
        """Gera conteúdo otimizado para .gitattributes"""
        print(f"\n🔧 GERANDO .gitattributes")
        print("=" * 60)
        
        attributes_lines = [
            "# Git LFS Configuration - Auto-generated",
            "# Generated by ChromaDB Analyzer",
            f"# Total size: {self.total_size / (1024*1024):.2f} MB",
            ""
        ]
        
        # Regras específicas baseadas na análise
        
        # 1. Arquivos grandes específicos
        if self.large_files:
            attributes_lines.append("# Large files (>100MB)")
            for file in self.large_files:
                attributes_lines.append(f"{file['name']} filter=lfs diff=lfs merge=lfs -text")
            attributes_lines.append("")
        
        # 2. Extensões que precisam de LFS
        lfs_extensions = set()
        for ext, files in self.file_stats.items():
            # Se qualquer arquivo da extensão for > 50MB, toda extensão vai para LFS
            max_size = max(f['size_mb'] for f in files)
            total_size = sum(f['size_mb'] for f in files)
            
            if max_size > 50 or total_size > 100:
                lfs_extensions.add(ext)
        
        if lfs_extensions:
            attributes_lines.append("# Extensions requiring LFS")
            for ext in sorted(lfs_extensions):
                if ext:  # Ignora arquivos sem extensão por agora
                    attributes_lines.append(f"*{ext} filter=lfs diff=lfs merge=lfs -text")
            attributes_lines.append("")
        
        # 3. Pastas específicas do ChromaDB
        chroma_patterns = [
            "chroma_db/** filter=lfs diff=lfs merge=lfs -text",
            "*.sqlite3 filter=lfs diff=lfs merge=lfs -text",
            "*.db filter=lfs diff=lfs merge=lfs -text",
            "*.bin filter=lfs diff=lfs merge=lfs -text",
            "*.pickle filter=lfs diff=lfs merge=lfs -text",
            "*.parquet filter=lfs diff=lfs merge=lfs -text"
        ]
        
        attributes_lines.append("# ChromaDB specific patterns")
        attributes_lines.extend(chroma_patterns)
        attributes_lines.append("")
        
        # 4. Outros arquivos grandes comuns
        other_patterns = [
            "# Other large file patterns",
            "*.json filter=lfs diff=lfs merge=lfs -text",
            "*.model filter=lfs diff=lfs merge=lfs -text",
            "*.pkl filter=lfs diff=lfs merge=lfs -text",
            "*.h5 filter=lfs diff=lfs merge=lfs -text",
            "*.weights filter=lfs diff=lfs merge=lfs -text"
        ]
        
        attributes_lines.extend(other_patterns)
        
        return "\n".join(attributes_lines)
    
    def save_gitattributes(self, content):
        """Salva o arquivo .gitattributes"""
        gitattributes_path = Path(".gitattributes")
        
        # Backup se já existir
        if gitattributes_path.exists():
            backup_path = Path(".gitattributes.backup")
            gitattributes_path.rename(backup_path)
            print(f"📋 Backup criado: {backup_path}")
        
        with open(gitattributes_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"✅ Arquivo criado: {gitattributes_path}")
        return gitattributes_path
    
    def generate_cleanup_script(self):
        """Gera script para limpeza do Git cache"""
        script_content = f"""#!/bin/bash
# Script de limpeza para Git LFS
# Gerado automaticamente

echo "🧹 Limpando cache do Git..."

# Remove arquivos do cache
"""
        
        for ext, files in self.file_stats.items():
            for file in files:
                if file['size_mb'] > 10:  # Apenas arquivos > 10MB
                    script_content += f'git rm --cached "{file["path"]}" 2>/dev/null\n'
        
        script_content += f"""
echo "✅ Cache limpo!"

echo "📋 Adicionando .gitattributes..."
git add .gitattributes
git commit -m "Configure Git LFS for ChromaDB"

echo "📦 Adicionando arquivos com LFS..."
git add chroma_db/
git add *.json
git commit -m "Add ChromaDB files with Git LFS"

echo "🚀 Fazendo push..."
git push origin main

echo "✅ Deploy concluído!"
"""
        
        with open("cleanup_git.sh", 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # Versão Windows
        bat_content = script_content.replace('#!/bin/bash', '@echo off').replace('echo "', 'echo ').replace('"', '')
        with open("cleanup_git.bat", 'w', encoding='utf-8') as f:
            f.write(bat_content)
        
        print("📜 Scripts criados: cleanup_git.sh e cleanup_git.bat")

def main():
    print("🔍 ChromaDB Analyzer & Git LFS Configurator")
    print("=" * 60)
    
    # Permite especificar pasta diferente
    chroma_path = input("📁 Caminho para ChromaDB (Enter para './chroma_db'): ").strip()
    if not chroma_path:
        chroma_path = "./chroma_db"
    
    analyzer = ChromaDBAnalyzer(chroma_path)
    
    # Análise
    if not analyzer.analyze_directory():
        return
    
    # Mostra resultados
    analyzer.print_analysis()
    
    # Gera .gitattributes
    gitattributes_content = analyzer.generate_gitattributes()
    
    print(f"\n📄 CONTEÚDO DO .gitattributes:")
    print("-" * 40)
    print(gitattributes_content)
    
    # Pergunta se quer salvar
    save = input(f"\n💾 Salvar .gitattributes? (s/N): ").strip().lower()
    if save in ['s', 'sim', 'y', 'yes']:
        analyzer.save_gitattributes(gitattributes_content)
        analyzer.generate_cleanup_script()
        
        print(f"\n🚀 PRÓXIMOS PASSOS:")
        print("1. Execute: cleanup_git.bat (Windows) ou cleanup_git.sh (Linux/Mac)")
        print("2. Ou manualmente:")
        print("   git rm --cached -r chroma_db/")
        print("   git add .gitattributes")
        print("   git commit -m 'Configure Git LFS'")
        print("   git add chroma_db/")
        print("   git commit -m 'Add ChromaDB with LFS'")
        print("   git push origin main")
    
    # Salva relatório JSON
    report = {
        "analysis_date": "2025-09-05 17:27:20",
        "total_size_mb": analyzer.total_size / (1024*1024),
        "large_files": analyzer.large_files,
        "file_stats": dict(analyzer.file_stats),
        "recommendations": {
            "use_git_lfs": len(analyzer.large_files) > 0,
            "total_files": sum(len(files) for files in analyzer.file_stats.values()),
            "needs_cleanup": analyzer.total_size > 100 * 1024 * 1024
        }
    }
    
    with open("chroma_analysis_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, default=str)
    
    print(f"\n📊 Relatório salvo: chroma_analysis_report.json")

if __name__ == "__main__":
    main()